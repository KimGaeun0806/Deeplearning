# AI+X : 딥러닝 기말 프로젝트

## 마스크 착용 여부 판단

### (올바른 마스크 착용, 잘못된 마스크 착용, 마스크 미착용)

- 2016030482 기계공학부 강승협 (tmdguq7438@naver.com)
- 2020091321 미디어커뮤니케이션학과 김가은 (greencloud11@naver.com)
- 2021098040 경영학부 홍성래 (smallgaint@hanyang.ac.kr)
- 2017003754 경영학부 홍유택 (ghddbxor123@naver.com)

---

[https://youtu.be/qKKKFpEwN5A](https://youtu.be/qKKKFpEwN5A)

# Contents

1. Introduction
2. Deep Learning이란 무엇인가?

     2.1.  인공신경망 잇는 기계학습법

     2.2.  딥러닝의 두 갈래

 3.  Data set

      3.1.  컴퓨터에서 이미지 처리 방식

             3.1.1. 픽셀 좌표계(Pixel Image Coordinate System)

             3.1.2. 흑백 영상(Gray-scale Image)과 컬러 영상(Color Image)

3.2. 이미지 전처리 과정

 4.  Model

      4.1.  CNN

             4.1.1.  Convolution

             4.1.2.  Padding

             4.1.3.  Pooling

             4.1.4. 전체 구조

             4.1.5.  활성화 함수(Activation Function)

                       4.1.5.1. ReLU

                       4.1.5.2. Softmax

             4.1.6.  손실 함수(Loss Function)

                       4.1.6.1. Binary crossentropy

                       4.1.6.2. Categorical crossentropy

             4.1.7.  척도(Metrics)

4.2. 실행 과정

5.   Predict Model

5.1. 정확도 측정

5.2. Loss, Accuracy 그래프

5.3. 실생활에서의 사용

5.4.  Video 

6.   Discussion

7.   References

---

# 1. Introduction

2020년은 말 그대로 혼돈의 도가니였다. 전염성이 높은 코로나19(COVID-19)라고 불리는 새로운 바이러스가 창궐하여 2021년 6월 4일 현재까지 전세계에서 172,026,054명이 확진되었고, 3,698,661명이 코로나 바이러스로 인해 사망하였다. 전세계의 국가들은 코로나 바이러스의 확산을 막기 위해 다른 나라에서 입국하는 것을 까다롭게 관리하는 것 뿐만 아니라 국민들의 마스크 착용 및 위생, 사회적 거리 두기 등을 강조하였다. 그 중 가장 중요하고 확산을 예방하는데 중요한 것은 바로 '마스크 착용'이다.

하지만 사람들에게 '마스크 착용'은 짜증나고 답답한 일이다. 이러한 이유 때문에 몇몇 사람들은 마스크로 입만 가리고 코는 노출하는 대충 쓰기도 하고, 심지어 몇몇 사람들은 마스크를 아예 쓰지 않기도 한다. 따라서 이러한 사람들을 찾아내서 처벌을 하는 것이 코로나 바이러스 확산 방지와 사회 질서 유지를 위해서 상당히 중요하다. 하지만 마스크를 대충 쓰거나 안 쓴 사람들을 일일이 사람이 직접 찾아내는 것은 한계가 있다. 만약 사람이 아닌 AI가 직접 마스크를 착용한 사람, 마스크를 대충 착용한 사람, 마스크를 착용하지 않은 사람을 구분할 수 있다면, 이러한 한계를 극복할 수 있을 것이다. 이러한 AI를 딥러닝(Deep Learning)을 통해 어떻게 구현해 볼 수 있을까? 먼저 딥러닝이란 무엇인지에 대해 알아보자.

---

# 2. Deep Learning이란 무엇인가?

## 2.1.  인공신경망 잇는 기계학습법

딥러닝은 사물이나 데이터를 군집화하거나 분류하는 데 사용하는 기술이다. 예를 들어 컴퓨터는 사진만으로 개와 고양이를 구분하지 못한다. 하지만 사람은 아주 쉽게 구분할 수 있다. 이를 위해 ‘기계학습(Machine Learning)’이라는 방법이 고안됐다. 많은 데이터를 컴퓨터에 입력하고 비슷한 것끼리 분류하도록 하는 기술이다. 저장된 개 사진과 비슷한 사진이 입력되면, 이를 개 사진이라고 컴퓨터가 분류하도록 한 것이다.

데이터를 어떻게 분류할 것인가를 놓고 이미 많은 기계학습 알고리즘이 등장했다. ‘의사결정나무’나 ‘베이지안망’, ‘서포트벡터머신(SVM)’, ‘인공신경망’ 등이 대표적이다. 이 중 딥러닝은 인공신경망의 후예다.

딥러닝은 인공신경망의 한계를 극복하기 위해 제안된 기계학습 방법이다. 딥러닝의 시작은 인공신경망 역사와 맞닿아 있기 때문이다. 1943년, 미국 일리노이 의대 정신과 부교수였던 워렌 맥컬록은 당시 의대 학생이었던 제리 레트빈과 월터 피츠와 함께 ‘신경 활동에 내재한 개념들의 논리적 계산’이라는 제목의 논문을 발표한다. 이들은 이 논문에서 신경망을 ‘이진 스위칭’ 소자가 복잡하게 연결된 네트워크로 모형화했다. 인공신경망을 개념화한 최초의 논문이다.

딥러닝이 처음 제안된 때는 인공신경망이 탄생한 지 40여년이 지난 1980년대다. 캘리포니아 대학 심리학자와 컴퓨터 관련 학자들의 신경망 연구를 요약한 <PDP>라는 저서가 등장하면서부터다.

딥러닝이 부활의 신호탄을 쏘아올리게 된 건 2004년이다. 제프리 힌튼 교수가 RBM이라는 새로운 딥러닝 기반의 학습 알고리즘을 제안하면서 주목을 받기 시작했다. 곧바로 드롭아웃이라는 알고리즘도 등장해 고질적으로 비판받는 과적합 등을 해결할 수 있게 됐다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/1.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/1.png)

        ▲ ‘신경 활동에 내재한 개념들의 논리적 계산’, 1943, 워렌 맥컬록

## 2.2. 딥러닝의 두 갈래

딥러닝의 핵심은 분류를 통한 예측이다. 수많은 데이터 속에서 패턴을 발견해 인간이 사물을 구분하듯 컴퓨터가 데이터를 나눈다. 이 같은 분별 방식은 두 가지로 나뉜다. ‘지도 학습(supervised learning)’과 ‘비지도 학습(unsupervised learning)’이다. 기존 기계학습 알고리즘은 대부분 지도 학습에 기초한다. 지도 학습 방식은 컴퓨터에 먼저 정보를 가르치는 방법이다. 예를 들어 사진을 주고 “이 사진은 고양이”라고 알려주는 식이다. 컴퓨터는 미리 학습된 결과를 바탕으로 고양이 사진을 구분하게 된다.

비지도 학습은 이 배움의 과정이 없다. “이 사진이 고양이”라는 배움의 과정 없이 “이 사진이 고양이군”이라고 컴퓨터가 스스로 학습하게 된다. 지도 학습과 비교해 진보한 기술이며, 컴퓨터의 높은 연산 능력이 요구된다. 구글이 현재 비지도 학습 방식으로 유튜브에 등록된 동영상 중 고양이 동영상을 식별하는 딥러닝 기술을 개발한 상태다.

구글은 음성인식과 번역을 비롯해 로봇의 인공지능 시스템 개발에도 딥러닝 기술을 도입하고 있다. 대표적인 SNS 업체 페이스북은 딥러닝을 뉴스피드와 이미지 인식 분야에 적용하고 있다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/2.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/2.png)

        ▲ 페이스북의 ‘딥페이스’ 동작 원리

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/4.jpg](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/4.jpg)

        ▲ 유튜브 영상에서 고양이를 찾아내는 구글의 딥러닝 기술

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/5.jpg](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/5.jpg)

       ▲ 개 품종을 알려주는 MS의 ‘프로젝트 아담’

---

# 3. Data set

## 3.1. 컴퓨터에서 이미지 처리 방식

### 3.1.1. **픽셀 좌표계 (Pixel Image Coordinate System)**

편의상 픽셀 좌표계라고 썼지만, 보통은 영상 좌표계(Image Coordinate System)라고 불린다. 픽셀 좌표계는 우리가 실제 눈으로 보는 영상에 대한 좌표계로서 위 <그림 1>과 같이 이미지의 왼쪽상단(left-top) 모서리를 원점, 오른쪽 방향을 x축 증가방향, 아래쪽 방향을 y축 증가방향으로 한다. 그리고 픽셀 좌표계의 x축, y축에 의해 결정되는 평면을 이미지 평면 (image plane)이라 부른다.

기하학적으로 볼 때, 3D 공간상의 한 점 P = (X,Y,Z)는 카메라의 초점 (또는 렌즈의 초점)을 지나서 이미지 평면의 한 점 pimg = (x, y)에 투영(projection) 된다. 픽셀 좌표계의 단위는 픽셀(pixel)이며, 픽셀 좌표는 다음과 같이 소문자로 표기한다.

[https://t1.daumcdn.net/cfile/tistory/240AE44251EE408307](https://t1.daumcdn.net/cfile/tistory/240AE44251EE408307)

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled.png)

                                                            **<그림 3>**

### 3.1.2. 흑백 영상(Gray-scale Image)과 컬러 영상(Color Image)

- **흑백 영상(Gray-scale Image)**

흑백 영상은 각 픽셀의 밝기 값을 0~255 사이의 값으로 표현한 이미지이다. 이는 8bit=1Byte의 메모리를 사용한 것이며 아래 그림은 똑같은 사과 영상을 8bit의 흑백 영상으로 표현한 것이다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%201.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%201.png)

왼쪽의 빨간색 박스 안쪽의 픽셀의 밝기값들을 오른쪽에 출력하였다. 픽셀 값들의 범위가 0~255사이의 값인 것을 볼 수 있다. 픽셀의 밝기 값들을 어느 범위에서 표현할지에 따라 흑백 영상의 모습이 약간씩 달라진다. 가장 큰 값으로 표현한 범위가 0~255인 8bit로 표현한 흑백 영상이다.

대부분의 영상처리 알고리즘은 이 흑백 이미지에서 처리된다. 심지어는 딥러닝을 통한 얼굴 인식 등에도 컬러 영상이 아닌 흑백 영상이 학습 데이터로써 이용된다. 다음에 설명할 컬러 이미지는 표현방법이 약간 복잡하기 때문에 영상 처리를 적용하기가 까다롭고 복잡하다.

- **컬러 영상(Color Image)**

컬러 영상은 3개의 채널로 표현된다. 이때 3개의 채널은 빛의 3원색인 빨강(Red), 녹색(Green), 파랑(Blue)이다. 각 채널은 0~255사이의 값으로 빨강의 정도, 녹색의 정도, 파랑의 정도를 각각 나타낸다.

컬러 영상에서는 세 개의 기본 컬러를 조합하여 다양한 색깔을 만들어낼 수 있는데 각각의 8bit의 기본 컬러 3개를 조합하여, 256 x 256 x 256 = 16,777,216가지의 컬러를 만들어낼 수 있다.

아래 그림은 위의 사과 영상을 컬러 이미지로 표현한 것이다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%202.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%202.png)

원래의 빨간 사과 컬러 이미지를 RGB의 각 채널별 이미지로 표현하였다. 보다시피 사과는 빨간색인데, 세 개의 채널 중 R 채널이 가장 밝은 것을 볼 수 있다. 가장 밝다는 것은 높은 값들이 많이 분포해 있다는 의미이며, 이는 색깔이 빨간색일 확률이 높은 것이다. 실제로 위의 이미지의 (x=163, y=116)인 좌표의 RGB값들은 R=180, G=2, B=2 이다. 따라서 이 좌표의 픽셀의 색깔은 빨간색에 가깝다.

## 3.2. 이미지 전처리 과정

데이터셋 출처

[Face Mask Detection](https://www.kaggle.com/vijaykumar1799/face-mask-detection)

마스크 착용 수준에 따른 구분에 필요한 데이터 셋은 마스크를 정상적으로 쓴 사진, 마스크를 코는 노출하고 입만 가린 상태로 쓴 사진, 마스크를 전혀 쓰지 않은 사진이다. 이 사진들은 kaggle에서 이미지 데이터 셋을 구하여 사용하였다.

먼저 세가지 경로로 나누어 저장되어 있는 각 이미지 데이터 셋의 경로를 DATA_LIST라는 하나의 리스트로 저장하였다.

```python
from glob import glob
import natsort

#모든 사진의 경로를 리스트로 저장
PATH = 'mask_dataset/'

DATA_LIST = [] 
for forders in ["with_mask", "mask_weared_incorrect", "without_mask"]:
  IMAGE_PATH = PATH + forders
  part_data_list = glob(IMAGE_PATH + "/*")
  part_data_list = natsort.natsorted(part_data_list)

  DATA_LIST += part_data_list
```

그 다음 데이터 셋을 3가지 종류에 따라 숫자로 레이블링을 해주고 각각의 이미지 데이터 이름과 레이블 번호, 가로 길이, 세로 길이 이렇게 4가지 정보를 하나의  데이터 프레임으로 묶어 주었다.

데이터 셋에 레이블을 붙여줄 때 마스크를 정상적으로 착용한 것은 2, 마스크를 코는 노출하고 입만 가린 상태로 착용한 것은 1, 마스크를 착용하지 않은 것은 0으로 붙여주었다. 그리고 뒤에서 이미지 데이터의 크기도 비교하기 위해 이미지의 가로와 세로 길이도 함께 묶었다.

```python
import numpy as np
import cv2
import pandas as pd

#with mask = 2, mask weared incorrect = 1, withgout mask = 0

category = []
xshape = []
yshape = []
data1 = []
label = []

for data in DATA_LIST:
    if data.split('_')[1] == 'dataset/with':
        category.append(2)
    elif data.split('_')[1] == 'dataset/without':
        category.append(0)
    else:
        category.append(1)
    img_array = np.fromfile(data, np.uint8)
    img = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)
    attribute = np.shape(img)
    xshape.append(attribute[1])
    yshape.append(attribute[0])
    
df = pd.DataFrame({
    'filename': DATA_LIST,
    'category': category,
    'x size': xshape,
    'y size': yshape
})

df.filename = df.filename.astype(str)
df.category = df.category.astype(str)
```

데이터 프레임의 첫 5줄과 마지막 5줄을 출력하면 이와 같다.

```python
df.head()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%203.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%203.png)

```python
df.tail()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%204.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%204.png)

각 레이블 별 데이터 셋의 갯수를 비교해보기 위해 레이블별로 그래프를 출력했다.

```python
import matplotlib.pyplot as plt
import seaborn as sns

#레이블별 데이터 갯수
sns.countplot(x='category', data=df)
plt.title("Data counts for labels")
plt.xticks(rotation=0)
plt.show()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%205.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%205.png)

위 그래프를 보면 레이블 별로 같은 개수의 데이터가 있음을 알 수 있다. 세 레이블 모두 2994개의 이미지 데이터가 있다.

각 레이블 별로 샘플 이미지를 하나씩 출력한 모습이다. 

```python
#각각의 샘플 이미지
import os
import random
from keras.preprocessing.image import  load_img

sample = random.choice(os.listdir(PATH + "with_mask/"))
image = load_img(PATH + "with_mask/" + sample)

plt.subplot(1,3,1)
plt.title('Correct mask')
plt.imshow(image)

sample = random.choice(os.listdir(PATH + "mask_weared_incorrect/"))
image = load_img(PATH + "mask_weared_incorrect/" + sample)

plt.subplot(1,3,2)
plt.title('Incorrect mask')
plt.imshow(image)

sample = random.choice(os.listdir(PATH + "without_mask/"))
image = load_img(PATH + "without_mask/" + sample)

plt.subplot(1,3,3)
plt.title('No mask')
plt.imshow(image)
plt.show
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%206.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%206.png)

왼쪽부터 2로 레이블링한 정상적으로 마스크를 착용한 모습, 1로 레이블링한 코를 드러내고 착용한 모습, 0으로 레이블링한 마스크를 착용하지 않은 모습이다. 세 이미지 파일 모두 앞에서 설명한 이미지의 좌표 위에 나타내었다.

각각의 이미지의 가로와 세로 길이를 비교해보았다.

```python
#이미지의 가로 길이 비교
plt.subplot(221)
sns.countplot(x='x size', palette='bright', data=df)
plt.title("Image width")
plt.xticks(rotation=0)
plt.xlabel(None)
plt.ylabel(None)

#이미지의 세로 길이 비교
plt.subplot(222)
sns.countplot(x='y size',palette='dark', data=df)
plt.title("Image height")
plt.xticks(rotation=0)
plt.xlabel(None)
plt.ylabel(None)

plt.show()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%207.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%207.png)

사용한 데이터 셋의 모든 이미지가 가로 128, 세로 128으로 크기가 같았기 때문에 따로 이미지를 리사이즈하거나 재구성 하는 등의 재가공하는 과정은 필요하지 않았다.

---

# 4. Model

## 4.1. CNN

`CNN`은 이미지를 인식하기 위해 패턴을 찾는데 특히 유용하다. 데이터에서 직접 학습하고 패턴을 사용해 이미지를 분류한다. 즉, 특징을 수동으로 추출할 필요가 없다.이러한 장점 때문에 자율주행 자동차, 얼굴인식과 같은 객체인식이나 computer vision이 필요한 분야에 많이 사용되고 있다.

- **CNN이 유용한 이유**

`CNN`이 나오기 이전, 이미지 인식은 2차원으로 된 이미지(채널까지 포함해서 3차원)를 1차원배열로 바꾼 뒤 `FC(Fully Connected)`신경망으로 학습시키는 방법이었다.

![https://user-images.githubusercontent.com/15958325/58844125-bde86180-86b0-11e9-8d58-5d068c26233e.png](https://user-images.githubusercontent.com/15958325/58844125-bde86180-86b0-11e9-8d58-5d068c26233e.png)

위와 같이 이미지의 형상은 고려하지 않고, raw data를 직접 처리하기 때문에 많은 양의 학습 데이터가 필요하고 학습 시간이 길어진다.

또한 이미지가 회전하거나 움직이면 새로운 입력으로 데이터를 처리해줘야 한다. 이미지의 특성을 이해하지 못하고 단순 1D 데이터로 보고 학습을 하는 것이 특징이다.

이러한 방법은 이미지 데이터를 평면화 시키는 과정에서 공간정보가 손실될 수밖에 없다. 즉, 신경망이 특징을 추출하고 학습하는데 있어 비효율적이고 정확도를 높이는데 한계가 있다.

이런 단점을 보완하여 이미지의 공간정보를 유지한채 학습을 하게하는 모델이 `CNN`이다.

- **CNN 작동법**

`CNN`의 가장 핵심적인 개념은 이미지의 공간정보를 유지한채 학습을 한다는 것이다. 용어를 설명하면서 CNN의 구성요소들이 어떻게 동작하는지 알아보자.

### 4.1.1. **Convolution**

`Convolution`의 사전적 정의는 합성곱이다. 단순히 이렇게만 들으면 의미를 잘 이해하지 못할수 있는데 알고보면 이 알고리즘을 가장 잘 표현한 용어라 볼 수 있다. 사실 `Convolution`은 처음 등장한 개념이 아니라 `CNN`이 등장하기 한참 전부터 이미지처리에서 사용되었던 개념이다.

그림으로 보면 다음과 같다.

![https://user-images.githubusercontent.com/15958325/58780750-defb7480-8614-11e9-943c-4d44a9d1efc4.gif](https://user-images.githubusercontent.com/15958325/58780750-defb7480-8614-11e9-943c-4d44a9d1efc4.gif)

위 gif의 첫번째 단계를 그림으로 보면 다음과 같다.

![https://user-images.githubusercontent.com/15958325/58845860-ca23ed00-86b7-11e9-805f-ef5c8adcab9f.png](https://user-images.githubusercontent.com/15958325/58845860-ca23ed00-86b7-11e9-805f-ef5c8adcab9f.png)

빨간 박스는 필터가 적용될 영역이고 개발자의 임의로 gif처럼 한칸씩 이동시키면서 적용시킬건지, 두칸씩 이동할건지 정할 수 있다.(이 값을 `stride`라고 한다.)

이를 통해 이미지의 `feature map`을 만들 수 있다. `filter`(또는 `kernel`)의 구성에 따라 이미지의 특징을 뽑을 수 있다.

- **Filter(Kernel)**

Filter라는걸 통해서 어떻게 이미지의 feature를 뽑을 수 있는지 예시를 통해 보자. 이미지처리에서 자주 등장하는 `sobel filter`는 이미지의 가로세로 feature를 뽑아낼 수 있는 필터이다.

필터를 보면 이런 값을 가지고 있다.

![https://user-images.githubusercontent.com/15958325/98335704-cd5b9f00-2048-11eb-8d3d-d3490e17cf46.PNG](https://user-images.githubusercontent.com/15958325/98335704-cd5b9f00-2048-11eb-8d3d-d3490e17cf46.PNG)

![https://user-images.githubusercontent.com/15958325/58784251-ed01c300-861d-11e9-9e6f-9b30bb1d5d7a.png](https://user-images.githubusercontent.com/15958325/58784251-ed01c300-861d-11e9-9e6f-9b30bb1d5d7a.png)

                                                               원본이미지

원본 이미지에 위 필터를 차례대로 적용시키면 다음과 같은 결과를 확인할 수 있다.

![https://user-images.githubusercontent.com/15958325/58784105-97c5b180-861d-11e9-8b25-deab83350d99.png](https://user-images.githubusercontent.com/15958325/58784105-97c5b180-861d-11e9-8b25-deab83350d99.png)

                                (왼쪽: `sobel-x`적용, 오른쪽: `sobel-y`적용)

각 이미지의 특징을 보니 왼쪽은 세로선이 감지되고 오른쪽은 가로선이 감지된 것을 확인할 수 있다.이 둘을 합치면 아래와 같이 원본사진의 feature를 확인할 수 있다.

![https://user-images.githubusercontent.com/15958325/58784450-6b5e6500-861e-11e9-8a2a-55bb0f2a9c44.png](https://user-images.githubusercontent.com/15958325/58784450-6b5e6500-861e-11e9-8a2a-55bb0f2a9c44.png)

이와 같이 `CNN`에서도 여러개 필터를 이용해 이미지의 세부 특징을 추출해서 학습을 할 수 있다.

위와 같은 이미지 처리에서는 sobel 필터와 같이 유명한 필터들을 직접 사용자가 찾아서 사용해야 했지만, **`CNN`은 신경망에서 학습을 통해 자동으로 적합한 필터를 생성해 준다는 것**이 특별하다.

### 4.1.2.**Padding**

Convolution 파트를 잠시 떠올려보자.

![https://user-images.githubusercontent.com/15958325/58845937-20912b80-86b8-11e9-8aaa-92fdaa3bbf10.png](https://user-images.githubusercontent.com/15958325/58845937-20912b80-86b8-11e9-8aaa-92fdaa3bbf10.png)

`Convolution` 레이어에서는 Filter와 Stride의 작용으로 `Feature map`의 크기는 입력 데이터 보다 작다.

Stride: Filter를 몇 칸 이동할건지 정한다.

이렇게 입력 데이터보다 출력 데이터가 작아지는 것을 방지하는 방법이 `Padding`이다.

![https://user-images.githubusercontent.com/15958325/58846398-ff313f00-86b9-11e9-8268-7989df7d38f2.png](https://user-images.githubusercontent.com/15958325/58846398-ff313f00-86b9-11e9-8268-7989df7d38f2.png)

위와 같이 0으로 둘러싸는 `padding`을 `zero padding`이라고 부른다. 단순히 0을 덧붙였으므로 특징이나 분해능에는 영향을 미치지 않는다.

이렇게 `padding`을 하게 되면 `convolution`을 해도 크기가 작아지지 않는다.

### 4.1.3. **Pooling**

이미지의 크기를 계속 유지한 채 `FC(Fully Connected)`레이어로 가게 된다면 연산량이 기하급수적으로 늘 것이다.

**적당히 크기도 줄이고, 특정 feature를 강조**할 수 있어야 하는데 그 역할을 `Pooling` 레이어에서 하게 된다.

처리 방법은 총 세 가지가 있다.

- **Max Pooling**
- Average Pooling
- Min Pooling

CNN에서는 주로 Max Pooling을 사용한다. 왜냐하면 뉴런이 가장 큰 신호에 반응하는 것과 유사하기 때문이다. 이렇게 하면 노이즈가 감소하고 속도가 빨라지며 영상의 분별력이 좋아진다.

![https://user-images.githubusercontent.com/15958325/58851117-60620e00-86cc-11e9-9b68-ce400aa93de0.png](https://user-images.githubusercontent.com/15958325/58851117-60620e00-86cc-11e9-9b68-ce400aa93de0.png)

최댓값을 구하거나 평균을 구하는 방식으로 동작한다. 일반적으로 pooling의 크기와 stride의 크기를 같게 설정하여 모든 원소가 한번씩은 처리가 되도록 설정한다.

특징은 다음과 같다.

- 학습대상 파라미터가 없음
- Pooling레이어를 통과하면 행렬의 크기가 감소함
- Pooling레이어를 통과해도 채널의 수는 변경없음

### 4.1.4. **전체 구조**

지금까지 CNN의 구성 요소들을 살펴보았다. 전체적인 그림을 보면 다음과 같다.

![https://user-images.githubusercontent.com/15958325/58851737-d9fafb80-86ce-11e9-8119-11876bc9e86c.png](https://user-images.githubusercontent.com/15958325/58851737-d9fafb80-86ce-11e9-8119-11876bc9e86c.png)

`Input` : 이미지

**특징 추출 단계(Feature Extraction)**

- `Convolution Layer` : 필터를 통해 이미지의 특징을 추출
- `Pooling Layer` : 특징을 강화시키고 이미지의 크기를 줄임.(Convolution과 Pooling을 반복하면서 이미지의 feature를 추출)

**이미지 분류 단계(Classification)**

- `Flatten Layer` : 데이터 타입을 FC네트워크 형태로 변경. 입력데이터의 shape 변경만 수행
- `Softmax Layer` : Classification수행
- `Output` : 인식결과

### 4.1.5. 활성화 함수(Activation Function)

![https://mblogthumb-phinf.pstatic.net/MjAyMDA0MTBfNjAg/MDAxNTg2NTA5MDQ5OTc4.jE8Z22hhetuxhfxBBT_cjSJg3ypIrkKnA8gzfM93pk8g.PsJV88sVs3vHGqhKKtDVviEBanirWWy9YMps-yVJp50g.PNG.qbxlvnf11/activation_function.png?type=w800](https://mblogthumb-phinf.pstatic.net/MjAyMDA0MTBfNjAg/MDAxNTg2NTA5MDQ5OTc4.jE8Z22hhetuxhfxBBT_cjSJg3ypIrkKnA8gzfM93pk8g.PsJV88sVs3vHGqhKKtDVviEBanirWWy9YMps-yVJp50g.PNG.qbxlvnf11/activation_function.png?type=w800)

활성화 함수(activation function)는 신경망의 output을 결정하는 식(equation)이다. 

각 뉴런은 가중치(weight)를 가지고 있으며 이것은 input number와 곱해져 다음 레이어로 전달하게 된다. 이때, 활성화 함수는 현재 뉴런의 input을 feeding 하여 생성된 output이 다음 레이어로 전해지는 과정 중 역할을 수행하는 수학적인 게이트(gate)라고 할 수 있다.

이 함수들은 신경망의 각 뉴런(neuron)에 붙어 있으며 뉴런의 input이 모델의 예측과 관련이 있는지 없는지를 근거로 이것을 활성화할지 활성화하지 않을지 결정한다.

또한, 활성화 함수는 각 뉴런의 output을 0과 1 또는 -1과 1 사이로 normalization 하여 모델이 복잡한 데이터를 학습하는 데 도움을 준다**.**

활성화 함수에는 이진 활성화 함수, 선형 활성화 함수, 비선형 활성화 함수의 세 가지 종류가 있다.

결론적으로 말하자면, 이진 활성화 함수, 선형 활성화 함수 모두 큰 문제점이 존재하기에 비선형 활성화 함수만 사용되는 추세이다.

따라서 활성화 함수 중 가장 많이 사용되는 비선형 함수인 렐루 활성화 함수에 대해 알아보자.

**4.1.5.1. 렐루 활성화 함수(ReLU)**

![https://mblogthumb-phinf.pstatic.net/MjAyMDA0MTBfMTEz/MDAxNTg2NTIzMDc5NjA3.thSvAKo4VF__Ltovh04xTUG0JIALY34AKuRxiccOJOog.giyisn8R9WD1-R2zzLdOtoADuAQL5Khn0bny1hCVxfsg.PNG.qbxlvnf11/%EF%BB%BFReLU.png?type=w800](https://mblogthumb-phinf.pstatic.net/MjAyMDA0MTBfMTEz/MDAxNTg2NTIzMDc5NjA3.thSvAKo4VF__Ltovh04xTUG0JIALY34AKuRxiccOJOog.giyisn8R9WD1-R2zzLdOtoADuAQL5Khn0bny1hCVxfsg.PNG.qbxlvnf11/%EF%BB%BFReLU.png?type=w800)

ReLU 함수의 식은 다음과 같다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%208.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%208.png)

****

가장 많이 사용되는 활성화 함수 중 하나이다.

Rectified Linear Unit Activation Function 즉, 개선된 선형 함수의 이름에 알맞듯이 값이 0보다 작거나 같으면 0, 0보다 크면 선형 함수에 값을 대입한다.

****

일부 정보는 과감히 무시하고 일부 정보는 수용하는 것을 통해 다른 활성화 함수에 비해 효율적인 결과를 보인다.

****

- 장점: 빠른 연산

단지 비교 연산 1회를 통해 값을 구한다.

다른 활성화 함수에 비해 수렴 속도가 매우 빠르다.

****

- 단점: 음수에 대한 대응

값이 음수일 경우 학습을 하지 못한다.

**4.1.5.2. 소프트맥스 함수(Softmax Function)**

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%209.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%209.png)

소프트맥스는 세 개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수이다. 분류되는 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.

소프트맥스 함수 식은 다음과 같다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2010.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2010.png)

- n= 출력층의 뉴런 수(총 클래수의 수), k=k번째 클래스
- 만약, 총 클래스의 수가 3개라고 한다면 다음과 같은 결과가 나오게 된다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2011.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2011.png)

- 위 공식을 보면, 소프트맥스 함수의 생김새는 "k번일 확률/전체 확률"로 단순하다는 것을 알 수 있다.

소프트맥스 함수는 출력층에서 주로 사용되며, 이중 분류가 아닌 다중 분류에서 주로 사용된다. 무엇보다도 소프트맥스 함수의 큰 장점은 확률의 총합이 1이므로 ,어떤 분류에 속할 확률이 가장 높을지를 쉽게 인지할 수 있다.

### 4.1.6. 손실 함수(Loss Function)

손실 함수란 딥러닝 혹은 머신러닝은 컴퓨터가 가중치를 찾아가는 과정이다.

아래 예를 통해 이해해보도록 하자.

```
4 = 2a + b
6 = 3a + b
```

위와 같은 두 연립 방정식이 있다고 하자. 사람들에게 a와 b에 들어가야 되는 답은 무엇인가? 라고 물어본다면

1. 값을 대입해서 문제를 풀어본다던지
2. 직관적으로 풀어본다던지
3. 여러가지 공식을 써서 풀어본다던지

할 것이다. 2번과 3번과 같은 경우 컴퓨터에게 시키기에는 매우 힘든 작업이다. 반대로 값이 엄청 많을 경우 1번은 인간에게 힘들 수 있다. 물론 위의 문제는 너무 쉽기 때문에 값을 대충 대입해서도 충분히 맞출 것이다.

컴퓨터는 기본적으로 첫번째 방법인 값을 대입해서 문제를 풀어본다. 그래서 대입한 결과와 실제 정답간의 간격 즉, 차이를 최대한 줄이는 방향으로 값을 대입하게 된다. 이 값의 차이를 loss라고 하며, 이 loss를 줄이는 방향으로 학습이 진행이 된다.

손실 함수의 종류에는 MSE, RMSE, Binary Crossentropy, Categorical Crossentropy 등 여러가지가 있지만 그 중에서 우리의 주제와 밀접하게 관련된 Binary Crossentropy, Categorical Crossentropy에 대해 알아보도록 하자.

**4.1.6.1. Binary Crossentropy**

만약 이진 분류기를 훈련하려면, binary crossentropy 손실함수를 사용하면 된다. 이진 분류기라는 것은 True 또는 False, 양성 또는 음성 등 2개의 클래스를 분류할 수 있는 분류기를 의미한다. binary crossentropy는 다음과 같은 공식으로 쓸 수 있다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2012.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2012.png)

손실함수는 예측값과 실제값이 같으면 0이 되는 특성을 갖고 있어야 한다. 예측값과 실제값이 모두 1로 같을 때, 손실함수값이 0이 되는지 한번 확인해보자. 

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2013.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2013.png)

참고로 이진 분류기의 경우 예측값이 0과 1사이의 확률값으로 나온다. 1에 가까우면 하나의 클래스(예를 들어, True 클래스)일 확률이 큰 것이고, 0에 가까우면 다른 하나의 클래스(예를 들어, False 클래스)일 확률이 큰 것이다. 상황을 간단하게 하기 위해서 샘플이 하나만 있다고 가정하자.

                                L=−[1log1+(1−1)log(1−1)]=0

예측값과 실제값이 같은 경우에는 예상했던 대로 손실 함수값은 0이 된다. 이번에는 예측값은 0, 실제값은 1인 상황에는 어떻게 되는지 확인해보자

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2014.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2014.png)

        L=−[1log0+(1−1)log(1−0)]=∞L=−[1log0+(1−1)log(1−0)]=∞

양의 무한대가 된다. 그런데 일반적으로 확률이 0이 나오지는 않기 때문에 꽤 큰 수가 나온다고 생각하면 된다. 이러한 특성을 갖고 있기 때문에 binary crossentropy가 이진 분류에 적절히 사용될 수 있는 손실 함수인 것이다.

**4.1.6.2. Categorical Crossentropy**

categorical crossentropy는 이번 주제의 마스크를 착용한 사람, 마스크를 대충 쓴 사람, 마스크를 착용하지 않은 사람처럼 분류해야할 클래스가 3개 이상인 경우, 즉 멀티클래스 분류에 사용된다. 라벨이 [0,0,1,0,0], [1,0,0,0,0], [0,0,0,1,0]과 같이 one-hot 형태로 제공될 때 사용된다. 공식은 다음과 같다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2015.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2015.png)

여기서 C는 클래스의 개수이다.

이번에도 샘플이 하나만 있다고 가정하고, 실제값과 예측값이 완전히 일치하는 경우의 손실 함수값을 살펴보자. 실제값과 예측값이 모두 [1 0 0 0 0]이라고 가정한다.

                  L=−(1log1+0log0+0log0+0log0+0log0)=0

계산했더니 0이 나왔다. 이번에는 실제값은 [1 0 0 0 0], 예측값은 [0 1 0 0 0]인 경우의 손실 함수값을 구해보자.

                   L=−(1log0+0log1+0log0+0log0+0log0)=∞

계산했더니 양의 무한대가 나왔다. 일반적으로 예측값은 [0.02 0.94 0.02 0.01 0.01]와 같은 식으로 나오기 때문에 양의 무한대가 나올리는 없지만, 큰 값이 나오는 것만은 분명하다. 이러한 특성을 가지고 있기 때문에 categorical crossentropy는 멀티클래스 분류 문제의 손실함수로 사용되기에 적합하다.

### 4.1.7. 척도(Metrics)

척도라는 것은 어떤 모델을 평가(Evaluate)하기 위해서 사용하는 값이다. 그러다보니 비슷한 개념의 목적/손실함수(Loss Function)와의 개념이 헷갈릴 수 있다.

손실 함수는 모델의 성능을 끌어올리기 위해서 참조하는 값이다. 즉, 트레이닝(training, 학습)을 위해서만 사용하는 나침반과 같은 존재라고 한다면, 척도는 결과적으로 이 모델의 성능은 얼마짜리야라는 개념이다.

그렇기 때문에 척도와 손실 함수의 개념은 분리되어야 하지만 경우에 따라서는 특정 척도에 따라 최적화 하는 것이 최상일 때도 있다. 둘간의 개념이 사실상 유사하기 때문에 동일한 개념으로 어쩔땐 척도를 사용하기도 하고 어쩔땐 손실함수를 쓰기도 한다.

좀 더 자세히 말하면, Loss function은 model performance의 측정과 machine learning model을 학습시키기 위해서 사용되는 함수 이다. 다시말하면 최적화를 위해 사용되는 함수이다. 그렇기 때문에 모델의 parameters(매개변수)들이 미분 가능해야 한다.

반면에, Metrics의 경우 모델의 학습 또는 테스트 과정에서 model performance를 확인하고 평가만을 위해 사용되기 때문에 미분이 불가능해도 상관없다.

몇몇 상황의 경우 미분가능한 Metrics(such as MSE)는 loss function으로 사용되기도 한다.

척도의 종류로 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 등이 있는데 그 중 가장 많이 쓰이는 개념인 정확도에 대해 알아보자.

**정확도(Accuracy)**

사실상 가장 많이 쓰이는 개념으로 타겟 대비 정확히 예측한 비율을 정의한다.

```
Accuracy = (TP + TN) / (TP + FP + FN + TN)
```

Classification accuracy는 단순한 metrics 중 하나 이다. 단순히 전체 예측 개수 중에 맞춘 예측 개수로 나누어 100을 곱한 것을 말한다. 흔히 말하는 얼마나 많이 맞췄냐만 생각하는 것이다.

## 4.2 실행 과정

이미지를 학습을 위해서 데이터 셋을 분리시킨 후 전처리 과정을 거친다.

```python
from sklearn.model_selection import train_test_split
import tensorflow as tf

#데이터 셋을 train, test 셋으로 분리
train_df, test_df = train_test_split(df, train_size=0.7, shuffle = True, random_state=14)

# 데이터 셋의 스케일을 줄이고 train셋에서 validation셋을 분리
train_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1./255,
    validation_split = 0.2
)

test_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1./255
)

#train, validation, test 셋을 훈련시키기 위한 전처리 과정
train_img = train_generator.flow_from_dataframe(
    dataframe = train_df,
    x_col = 'filename',
    y_col = 'category',
    target_size = (128,128),
    color_mode = 'rgb',
    class_mode = 'categorical',
    batch_size = 32,
    shuffle = True,
    seed =42,
    subset = 'training'
)
val_img = train_generator.flow_from_dataframe(
    dataframe = train_df,
    x_col = 'filename',
    y_col = 'category',
    target_size = (128,128),
    color_mode = 'rgb',
    class_mode = 'categorical',
    batch_size = 32,
    shuffle = True,
    seed = 42,
    subset = 'validation'
)
test_img = test_generator.flow_from_dataframe(
    dataframe = test_df,
    x_col = 'filename',
    y_col = 'category',
    target_size = (128,128),
    color_mode = 'rgb',
    class_mode = 'categorical',
    batch_size = 32,
    shuffle = False
)
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2016.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2016.png)

여기서 사용된 모델은 mobilenet이다. mobilenet은 Depth wise Convolution(채널을 분리해내 따로 CNN을 계산을 하고 다시 합치는) 기법을 통해 연산량을 줄여 경량화한 모델이다.

```python
import tensorflow.keras as keras
from tensorflow.keras.layers import Input
from sklearn.model_selection import train_test_split

baseModel = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(128, 128, 3)))

# mobilenet의 classifier(분류기)
headModel = baseModel.output
# Pooling layer
headModel = keras.layers.AveragePooling2D(pool_size=(4, 4))(headModel)
# Flatten함수를 이용하여 1차원 배열 
headModel = keras.layers.Flatten(name='flatten')(headModel)
# Activation함수는 relu 
headModel = keras.layers.Dense(128, activation='relu')(headModel) 
# Overfitting이 되지 않기 위해 dropout
headModel = keras.layers.Dropout(0.5)(headModel)
# 3개의 클래스 출력하므로 softmax 사용
headModel = keras.layers.Dense(3, activation='softmax')(headModel) 
# model 설정
model = keras.models.Model(inputs=baseModel.input, outputs=headModel) 

for layer in baseModel.layers:
  layer.trainable = False
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2017.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2017.png)

학습을 시작하는 과정이다. epochs를 50, batch size를 32로 한 후 학습을 진행하였다.

epochs는 조절하면서 결과를 관찰할 것이다.

```python
EPOCHS = 50
BATCH_SIZE = 32

# Loss function은 categorical_crossentropy, optimizer은 adam, metrics는 accuracy
model.compile(loss = 'categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

# 학습
H = model.fit(train_img,
              validation_data=val_img,
              batch_size = BATCH_SIZE,
              epochs = EPOCHS)

# 모델 저장
model.save('final_mask_detector', save_format='h5')
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2018.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2018.png)

# 5. Predict Model

## 5.1. 정확도 측정

test셋으로 모델이 얼마나 정확한지 확률을 계산한다.

```python
# test셋으로 정확도 측정
_, acc = model.evaluate(test_img, verbose = 0)
print("Accuracy is = ", (acc * 100.0), "%")
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2019.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2019.png)

epochs를 10, 30, 50으로 설정하여 각각의 accuracy를 측정했다.

이 과정에서 epochs가 늘어날 수록 정확도가 높아지는 것을 알 수 있다.

[ ](https://www.notion.so/6c6b843f01b649058be6a3f2cffbc049)

## 5.2. Loss, Accuracy 그래프

epochs를 50으로 선택을 하고 다음을 진행하였다.

학습과정에 대한 loss값과 accuracy를 그래프로 출력한다.

```python
# 모델 학습 loss, accuracy 그래프 출력
N = EPOCHS
plt.style.use("default")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
plt.show()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2020.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2020.png)

## 5.3. 실생활에서의 사용

### 5.3.1. Image

실생활에서 사용하기 위해 외부 사진을 가져온 후 평가해보았다.

```python
import cv2
from keras.models import  load_model
from keras.preprocessing.image import img_to_array, loda_img

# 모델 불러오기
model = load_model('final_mask_detector')

# 예측할 사진을 불러와서 평가하기 
plt.figure()
i=1

test_image_path = "test/"
for filename in ["1.jpg","2.jpg","3.jpg","4.jpg","5.jpg","6.jpg","7.jpg","8.jpg"]:
    
    img = load_img(test_image_path+filename, target_size=(128, 128))
    img = img_to_array(img)
    img = img / 255
    img = np.expand_dims(img, axis=0)
    new_pre = model.predict(img)
    print(new_pre)

    new_pre = new_pre.argmax(axis=-1)
    if new_pre == 2:
        title = "CORRECT"
    elif new_pre == 1:
        title = "INCORRECT"
    else:
        title = "NO"
        
    new_img = cv2.imread(test_image_path + filename)
    new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)
    new_img = cv2.resize(new_img, (128, 128), interpolation=cv2.INTER_AREA)
    
    plt.subplot(2,4,i)
    plt.title(title)
    plt.imshow(new_img)
    
    ax = plt.gca()
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    
    i+=1

plt.show()
```

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2021.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2021.png)

결과를 볼 수 있듯이 마스크 착용, 마스크 잘못 착용, 마스크 미착용 분류를 잘 했음을 확인 할 수 있었다.

### 5.3.2. Video

마스크 착용, 마스크 잘못 착용, 마스크 미착용 분류를 이미지가 아닌 영상으로도 분류할 수 있게 해보았다.

```python
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.models import load_model
import numpy as np
import cv2
import matplotlib.pyplot as plt
import os

# 얼굴을 찾아주는 모델 불러오기
facenet = cv2.dnn.readNet('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')

# 모델 불러오기
model = load_model('mask_classification')

# 컴퓨터 카메라로부터 이미지 받아오기
cap = cv2.VideoCapture(0)

while cap.isOpened(): 
    ret, img = cap.read()
    if not ret:
        break

# 프레임의 가로 세로 길이 저장하기 
h, w = img.shape[:2]

blob = cv2.dnn.blobFromImage(img, scalefactor=1., size=(300, 300), mean=(104., 177., 123.))
facenet.setInput(blob)
dets = facenet.forward()
```

얼굴인지 아닌지의 확률값을 가져온 후, 얼굴일 확률이 0.7 이하라면 아래 코드를 건너뛰고 계속한다. 그리고 이미지에서 얼굴만 잘라낸다.

```python
result_img = img.copy()

for i in range(dets.shape[2]):
		confidence = dets[0, 0, i, 2] 
		if confidence < 0.7: 
				continue

		x1 = int(dets[0, 0, i, 3] * w)
    y1 = int(dets[0, 0, i, 4] * h)
    x2 = int(dets[0, 0, i, 5] * w)
    y2 = int(dets[0, 0, i, 6] * h)

		face = img[y1: y2, x1: x2]
```

모델의 입력 크기가 224*224이므로 얼굴 영상의 크기를 (224, 224)로 바꾸고 RGB 배열로 변경한다. 그리고 이미지 입력을 위해 차원을 하나 더 생성한다. 

3 * 224 * 224 ⇒ 1 * 3 * 224 * 224

```python
if face.size == 0:
	continue
face_input = cv2.resize(face, dsize=(224, 224)) 
face_input = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB) 
face_input = np.expand_dims(face_input, axis=0) 
```

얼굴 이미지에서 마스크 쓴 확률, 잘못 쓴 확률, 안 쓴 확률을 예측해내고 squeeze 함수를 통해 차원을 줄여준다.

```python
pred = model.predict(face_input).squeeze()

#각각의 확률 출력하기 
incorrect_mask, mask, nomask= pred 
print('incorrect mask: {:.3f}%,  mask: {:.3f}%,  unmask: {:.3f}%'.format(incorrect_mask * 100, mask * 100, nomask * 100))
```

각 클래스의 확률을 반환한다.

3개의 클래스 중 가장 확률이 높은 클래스로 레이블을 정한다.

레이블 별로 색을 다르게 해서 구분을 시켜준다.

```python
if nomask > incorrect_mask and nomask > mask: 
            color = (0, 255, 0)
            label = 'No Mask %d%%' % (nomask * 100)
elif incorrect_mask > mask and incorrect_mask > nomask:
            color = (0, 0, 255)
            label = 'Incorrect Mask %d%%' % (incorrect_mask * 100)
else:
            color = (255, 0, 0)
            label = 'Mask %d%%' % (mask * 100)
```

이미지에 사각형을 그려넣고, 텍스트를 삽입한다.

```python
# 레이블 텍스트 삽입하기 
cv2.rectangle(result_img, pt1=(x1, y1), pt2=(x2, y2), thickness=2, color=color, lineType=cv2.LINE_AA) 
cv2.putText(result_img, text=label, org=(x1, y1 - 10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=color, thickness=2, lineType=cv2.LINE_AA)

# q를 누르면 종료
cv2.imshow('result', result_img)
if cv2.waitKey(1) == ord('q'):
		break

cap.release()
```

결과이미지를 출력한다. 

실제 팀원들의 영상으로 테스트해보았다. 

[AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/KakaoTalk_20210612_162023400.mp4](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/KakaoTalk_20210612_162023400.mp4)

영상을 찍어 테스트해본 결과 마스크 미착용, 올바른 마스크 착용, 잘못된 마스크 착용이 정상적으로 인식되었다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2022.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2022.png)

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2023.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2023.png)

                🔺 마스크 미착용               🔺 올바른 마스크 착용       🔺 잘못된 마스크 착용

위의 사진들은 마스크 미착용, 올바른 마스크 착용, 잘못된 마스크 착용이 올바르게 판별된 예이다.

![AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2024.png](AI+X%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%86%E1%85%A1%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%2028cc0b3a9c1644c1874a4d8ef99b472e/Untitled%2024.png)

하지만 이와 같이 허공에 사람이 인식되거나, 마스크 착용이 올바르게 인식되지 않는 경우도 있었다.

# 6. Discussion

지금까지 딥러닝과 CNN에 대한 전반적인 이론을 살펴보았고, 딥러닝을 통해 마스크를 올바르게 착용한 사람, 마스크를 잘못 착용한 사람, 마스크를 착용하지 않은 사람을 구분해 낼 수 있는 프로그램을 만들어보았다.  위와 같이 세 가지로 잘 구별하는 경우도 있었지만 가끔은 오류가 생기는 경우도 있었다. 그렇다면 어떻게 하면 이런 오류를 줄이고 성능을 더 향상 시킬 수 있을까?

예를 들어, 컴퓨터에게 노란 바나나만을 학습 시킨다고 생각해보자. 학습량이 더 많이 늘어날수록 학습 데이터인 노란 바나나에 대해서는 오차가 줄어들 것이다. 하지만 역설적이게도 실제 데이터로써 연두색 바나나나 흑점이 있는 바나나를 보게 되면 바나나인 것을 인지하지 못할 것이다. 즉, 데이터에 대한 학습이 너무 많이 수행되는 현상으로, 학습 대상 데이터에 대한 오차는 감소하지만, 실제 사례에 적용할 경우 오차가 증가하는 현상인 과적합(Overfitting)이 발생하면 오히려 컴퓨터가 제대로 기능하지 못하게 될 것이다. 따라서 과적합이 안되는 선에서 ('5.1. 정확도 측정'에서 볼 수 있듯이)epochs를 증가해야 문제가 생기지 않고 정확도 즉, 성능이 좋아진다.

하지만, 맛있는 드레싱을 만들었다고 해서 맛있는 샐러드가 탄생되는 것은 아니다. 맛있는 드레싱을 아삭아삭한 신선한 채소에 뿌려줘야 비로소 완전한 맛있는 샐러드가 나오는 것이다. 지금까지 우리가 해온 것도 마찬가지다. 아무리 좋은 프로그램을 만들었다고 하더라도 좋은 곳에 제대로 쓰이지 않으면 의미가 없다. 처음 Introduction에서 의도한 바와 같이 일일이 확인할 수 없을 정도의 많은 사람들이 오가는 마트나 술집과 같은 곳이나 병원과 같이 위험하고 주의를 기울어야 하는 곳에서 마스크를 올바르게 착용한 사람, 마스크를 잘못 착용한 사람, 마스크를 착용하지 않은 사람을 구분하여 코로나 바이러스 확산 방지와 사회 질서를 유지하는데 쓰여야 비로소 의미가 생기는 것이다.

---

# 7. References

1. 중앙재난안전대책본부, 2021.6.4. [http://ncov.mohw.go.kr/bdBoardList_Real.do?brdId=1&brdGubun=14&ncvContSeq=&contSeq=&board_id=&gubun=](http://ncov.mohw.go.kr/bdBoardList_Real.do?brdId=1&brdGubun=14&ncvContSeq=&contSeq=&board_id=&gubun=)
2. 용어로 보는 IT, 오원석, 2014.8.14. [https://terms.naver.com/entry.naver?docId=3578519&cid=59088&categoryId=59096](https://terms.naver.com/entry.naver?docId=3578519&cid=59088&categoryId=59096)
3. 이미지에 대해서 알아보자, 2017.4.21. [https://ndb796.tistory.com/11](https://ndb796.tistory.com/11)
4. 디지털 이미지의 표현과 크기 계산법, 2017.1.17. [https://twlab.tistory.com/23](https://twlab.tistory.com/23)
5. 좌표계, 2013.7.6. [https://darkpgmr.tistory.com/77](https://darkpgmr.tistory.com/77)
6. CNN이란?, 정호빈, 2018.8.1. [https://hobinjeong.medium.com/cnn-convolutional-neural-network-9f600dd3b395](https://hobinjeong.medium.com/cnn-convolutional-neural-network-9f600dd3b395)
7. CNN 요약, 김태원, 2018.1.4. [http://taewan.kim/post/cnn/](http://taewan.kim/post/cnn/)
8. 호다닥 공부해보는 CNN, 2019.6.3. [https://gruuuuu.github.io/machine-learning/cnn-doc/](https://gruuuuu.github.io/machine-learning/cnn-doc/)
9. What is activation function?, 2019.9.15. [https://leedakyeong.tistory.com/entry/밑바닥부터-시작하는-딥러닝-활성화함수란-What-is-activation-function](https://leedakyeong.tistory.com/entry/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98%EB%9E%80-What-is-activation-function)
10. 소프트맥스 함수(Softmax Function), 2021.1.26. [https://gooopy.tistory.com/53?category=824281](https://gooopy.tistory.com/53?category=824281)
11. 목적/손실 함수 이해 및 종류, 2020.8.3. [https://needjarvis.tistory.com/567](https://needjarvis.tistory.com/567)
12. 딥러닝 손실 함수 정리, 2021.2.14. [https://bskyvision.com/822](https://bskyvision.com/822)
13. 척도의 설명 및 종류, 2020.8.3. [https://needjarvis.tistory.com/568](https://needjarvis.tistory.com/568)
14. Metrics란 - 1. confusion matrix, accuracy, precision, recall.., 2021.2.22. [https://supermemi.tistory.com/78](https://supermemi.tistory.com/78)

---

### 역할 분담

강승협 : 전반적인 code 담당, 모델 처리 부분 블로그 작성, 모델 발표

김가은 : model/video code 담당, 비디오 처리 부분 블로그 작성, 결과 및 실행 과정 발표, 영상 편집

홍성래 : image code 담당, 전처리 부분 블로그 작성, 이미지 전처리 발표

홍유택 : 이론 담당, 그 외 나머지 블로그 작성, 블로그 및 딥러닝 소개 발표

---